{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim import models, corpora\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import spacy\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1981\n",
    "end_year = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data from individual files into only giant list\n",
    "data = []\n",
    "for k in range(end_year - start_year):\n",
    "    try:\n",
    "        yearly_articles = (pd.read_csv(r'C:\\Users\\$ubhajit\\Downloads\\Technocolabs Project\\ExchangeRateForecast\\data\\news\\articles/articles_{}.csv'.format(start_year+k)))['article']\n",
    "        yearly_articles = [re.sub('\\s+', ' ', str(sent)) for sent in yearly_articles]\n",
    "        yearly_articles = [re.sub(\"\\'\", \"\", str(sent)) for sent in yearly_articles]\n",
    "        for article in yearly_articles:\n",
    "            data.append(article)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293238\n"
     ]
    }
   ],
   "source": [
    "NO_DOCUMENTS = len(data)\n",
    "print(NO_DOCUMENTS)\n",
    "NUM_TOPICS = 7\n",
    "STOPWORDS = stopwords.words('english')\n",
    "STOPWORDS.extend(['new', 'inc', 'like', 'one', 'two', 'inc.', 'nan'])\n",
    "\n",
    "# ps = PorterStemmer()\n",
    "lemm = WordNetLemmatizer()\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    " \n",
    "def clean_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = [clean_text(document) for document in data]\n",
    "\n",
    "data_lemmatized = lemmatization(tokenized_data, allowed_postags=['NOUN', 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(data_lemmatized)\n",
    "dictionary.save(r\"C:\\Users\\$ubhajit\\Downloads\\Technocolabs Project\\ExchangeRateForecast\\models\\saved_model_data\\lemmatized_dictionary.dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in data_lemmatized]\n",
    "corpora.MmCorpus.serialize(r\"C:\\Users\\$ubhajit\\Downloads\\Technocolabs Project\\ExchangeRateForecast\\models\\saved_model_data\\serialised_corpus.mm\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LDA model\n",
    "dictionary = corpora.Dictionary.load(r\"C:\\Users\\$ubhajit\\Downloads\\Technocolabs Project\\ExchangeRateForecast\\models\\saved_model_data\\lemmatized_dictionary.dic\")\n",
    "mm = corpora.MmCorpus(r\"C:\\Users\\$ubhajit\\Downloads\\Technocolabs Project\\ExchangeRateForecast\\models\\saved_model_data\\serialised_corpus.mm\")\n",
    "lda_model = models.ldamulticore.LdaMulticore(corpus=mm, random_state=100, num_topics=NUM_TOPICS, id2word=dictionary, workers=2)\n",
    "lda_model.save(r'C:\\Users\\$ubhajit\\Downloads\\Technocolabs Project\\ExchangeRateForecast\\models\\saved_model_data/lda_{}.model'.format(NUM_TOPICS))\n",
    "lda_model = models.LdaModel.load(r'C:\\Users\\$ubhajit\\Downloads\\Technocolabs Project\\ExchangeRateForecast\\models\\saved_model_data/lda_{}.model'.format(NUM_TOPICS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doc_topics(mapping, num_topics):\n",
    "    doc_topic_mapping = []\n",
    "    for index, doc in enumerate(mapping):\n",
    "        obj = {}\n",
    "        for i in range(num_topics):\n",
    "            obj['news_topic#{}'.format(i)] = 0\n",
    "        for topic in doc:\n",
    "            obj['news_topic#{}'.format(topic[0])] = 1\n",
    "        doc_topic_mapping.append(obj)\n",
    "    return pd.DataFrame(doc_topic_mapping)\n",
    "document_topics = doc_topics([lda_model.get_document_topics(item) for item in mm], NUM_TOPICS)\n",
    "\n",
    "\n",
    "documents = pd.DataFrame()\n",
    "for year in range(end_year - start_year):\n",
    "    filename = r'C:\\Users\\$ubhajit\\Downloads\\Technocolabs Project\\ExchangeRateForecast\\data\\news\\articles/articles_{}.csv'.format(start_year+year)\n",
    "    if documents.empty:\n",
    "        documents = pd.read_csv(filename)\n",
    "    else:\n",
    "        documents = pd.concat([documents, pd.read_csv(filename)], sort=False)\n",
    "\n",
    "documents = documents.reset_index(drop=True)\n",
    "combined_data = pd.concat([documents,document_topics], axis=1, sort=False).reset_index(drop=True)\n",
    "combined_data.to_csv('documents_to_topics_{}.csv'.format(NUM_TOPICS), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n",
      "Topic #0:  company deal firm investor executive share stock percent year investment business shareholder chief fund financial last large market private price public equity trump trading asset big cash management plan analyst time capital value bank merger oil partner board stake acquisition \n",
      "Topic #1:  company year time people business last service technology executive site medium good many big product network way customer video system internet industry online phone film thing user computer work - consumer chief software percent market world web apple device sale \n",
      "Topic #2:  percent year market bank rate price economy tax financial high last fund growth money economic low investor month government loan bond credit debt interest investment many big time stock crisis business income job large term policy country cost week mortgage \n",
      "Topic #3:  government official country political leader year military people group american last election force many state party attack right week member month chinese time former power - war security day russian campaign policy effort police recent public weapon foreign vote opposition \n",
      "Topic #4:  case law government court year lawyer people state former legal rule drug investigation federal company official information report employee public health time last worker decision prosecutor agency trial charge security many issue system money care question action program month policy \n",
      "Topic #5:  people year family woman many child man day police city time school area last life car local home young old country water hour resident attack place work death student government month week building official night way parent food small part \n",
      "Topic #6:  company quarter sale share revenue year net loss earning profit cent result income period fourth percent first third second cost charge accounting month early operation gain tax expense report analyst operating quarterly late fiscal estimate unit taxis business store canadian \n",
      "- - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, re.sub('[\\\"\\+\\s\\d\\.*]+', ' ', lda_model.print_topic(idx, 40)))\n",
    " \n",
    "print(\"- \" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
